\section{Project analysis and System requirements}
\phantomsection

\subsection{Problem Definition}
Data journalism is a new field in data science and it has been used  successfully in the last decade by competitive journalists from famous newspapers such as "The New York Time", "The Atlantic" etc. An average person daily consumes a continuous stream of information. People are also curios about comprehensible articles that offers a report regarding a chain of events happening for a longer period of time. Writing compelling stories is the most challenging work that can be done by a journalist. It involves a lot of hard work and data analysis, obviously it is not an easy task to do. The good aspect about computers is that a lot of processes can be automated including data analysis. The problem is that such tool is not present on the market of Republic of Moldova. OpenMedia project intends to solve this problem.

\subsection{Project Analysis}
The project OpenMedia is a platform used to provide visual models and statistic result based on online media sources from Republic of Moldova. It aims to collect the available articles found on the public sources and use them as a generic source for extracting useful results for further use in visualization tools. The features offered by OpenMedia will be tools for querying for the desired keywords, in order to construct frequency plots, with options to see to what sources it belongs to. OpenMedia will also have trend detection utilities. By selecting a time range, the most mentioned terms will be rendered in form of a bar chart, thus communicating the popularity relevance.

The platform will consist of two major parts.
\begin{enumerate}
    \item Data gathering platform, that does the following:
    \begin{itemize}
        \item Fetching the article pages;
        \item Extracting the article;
        \item Storing the article;
        \item Enriching with textual metadata.
    \end{itemize}
    \item Web Client platform, that implies the following:
    \begin{itemize}
        \item A simple and intuitive design;
        \item Launching asynchronous tasks;
        \item Receiving the finished;
        \item Visualization of results.
    \end{itemize}
\end{enumerate}

\subsubsection{Data Gathering Platform}
This part of the application is focused on gathering data, cleaning and preprocessing it. Simply put, all the heavy lifting of textual data preprocessing is done by this part of the platform. In the \mbox{figure \ref{data_gathering_mock}} is mocked the application's activity layers. It is important to deliver the data in a format easily operable by the application because the amount of data analysis techniques could easily increase. More specific, it is up to client platform to choose the data consumption techniques. On one hand it is not an optimized solution but on the other hand it offers flexibility. Data collection is a long and tedious process, that is why it should better run once. More than that it is advised that all the data changes should be managed in a separate copy. It is recommended to keep every data version in an immutable state. Redundancy is a better choice due to the fact that a project is prone to changes. Running the entire gathering operation from the bottom just in order to add a new feature is lethal to a business solution. That is why OpenMedia platform keeps each data stage in a separate copy.

The articles used by the platform are extracted from public sources of the online media from Republic of Moldova. These are Unimedia, Timpul and Publika. As a future goal is intended to include the other available resources, in order to make the platform media agnostic. Adding to the system as many resources as possible is the key part of OpenMedia concept. The idea is that media sources are usually biased by political influence, which means that a research in a political area could lead to a less viable result. Beyond that the platform would provide means to deduce predispositions to some governmental parties.

\begin{figure}[!ht]
\centering
\includegraphics[width=3.8cm]{1_data_gathering}
\caption{Data Gathering platform mockup}\label{data_gathering_mock}
\end{figure}

\subsubsection{Web Client Platform}
The client side will represent a web application, a minimalistic layer used for communication between user and the platform. The web infrastructure makes the application easy to access. The requirement to run the client side application is to have a modern web browser, for instance Google Chrome, Firefox etc. The application focuses on making available the following functionalities:
\begin{itemize}
    \item execute queries on the specified words;
    \item execute trend detection queries on the specified date range;
    \item display frequency line charts and trendy bar charts;
\end{itemize}

These are the basic functionalities provided by the client application. Here are also included details such as specifying the media source, in order to be able to compare the results and extract useful conclusions out of it. The goal is to make the user experience as natural as possible. Due to the fact that he number of functionalities are not various, creating a natural flow of events will not represent an essential problem. It should be taken into consideration that a decreased amount of functionalities does not mean a reduced amount of operations. The whole sequence of events is rather complex. It will be described step by step in the next pages of the report.

\begin{figure}[!ht]
\centering
\includegraphics[width=13cm]{1_app_mock_1}
\caption{Primary page view mockup}\label{app_mock_1}
\end{figure}

In order to make an expected view of the application, there are illustrated mockups of the web applications. In figure \mbox{\ref{app_mock_1}} is represented the main page. The user has an input box used to execute queries related to word frequency. Because the query execution latency is going to be pretty long, the application is designed to work in an asynchronous way. After the clicking the button, a notification will pop up informing that a query has started. Also a pop up notification will be displayed when there are news about the query result. In case of success, the result can be accessed by clicking on View Queries button. The user can click a specific word and as an outcome the application will transit to the result view. The mockup of the page can be observed in figure \mbox{\ref{app_mock_2}}. A plot is rendered that represents per month intensity of the word mentions. According to date range, the necessary conclusions can be drawn. For instance, when was the specific word at its peak. When it started to become less popular, what are the spikes, what caused the spikes. This might prove quite valuable information if examined from the right angle. The good part is that anyone interested of statistic results can use the application. Starting from business entrepreneurs that are trying to research the trendiness of a specific product, a journalist that is interested in popularity of a specific event or entity. A great aspect is that a personal set of queries can be executed, thus building a custom aggregation of results.

\begin{figure}[!ht]
\centering
\includegraphics[width=13cm]{1_app_mock_2}
\caption{Frequency plot view mockup}\label{app_mock_2}
\end{figure}

\subsection{Theoretical Analysis}
Considering the complexity of the platform, a right amount of research is required in order to construct a workable product. The project OpenMedia consists of two independent separate products. Both entities represents the same new evolving field, data science. Thee final product represents a workable application that can be used on a local market. Further will follow a more detailed description of various aspects of the platform, regarding the technologies best suited for building the application, the concepts behind different tools used in the application and means for solving specific problems.

\subsubsection{Data Journalism}
Some people think of data as any collection of numbers, most likely gathered on a spreadsheet. Twenty years ago, that was pretty much the only sort of data that journalists dealt with. But the explosion of technology in the last decades open the doors for the new opportunities. Nowadays almost everything can be expressed in numbers. Data can be the source of data journalism, or it can be the tool with which the story is told — or it can be both. Like any source, it should be treated with scepticism; and like any tool, people should be conscious of how it can shape and restrict the stories that are created with it. Data journalism reflects the increased interaction between content producers (journalist) and several other fields such as design, computer science and statistics.

The journalists main resource for their work is data, and it was so from the very begging of the journalism. The computer revolution from the 20th century revealed massive opportunities for a lot of economical fields (and not only). Journalism started integrating data by bringing computer technologies into their daily lives. The nowadays explosion of technologies such as internet, cloud computing, high performance of mobile devices, easy accessible open source software influenced a lot the modern journalism leading to emergence of data journalism. This new term represents a new extended branch of traditional journalism by involving more data in the field. Decades after early pioneers successfully applied computer-assisted reporting and social science to investigative journalism, journalists are creating news apps and interactive features that help people understand data, explore it, and act upon the insights derived from it. New business models are emerging in which data is a raw material for profit, impact, and insight, co-created with an audience that was formerly reduced to passive consumption.

Journalists around the world now are faced with exciting challenges of writing reports and stories based on deductions made on vast amount of data which is daily increasing and changing. The data generation source are diverse, network lives, businesses, government, politicians, popularities etc. While the potential of data journalism is immense, the pitfalls and challenges to its adoption throughout the media are similarly significant. Global threats to press freedom, digital security, and limited access to data create difficult working conditions for journalists in many countries. A combination of peer-to-peer learning, mentorship, online training, open data initiatives, and new programs at journalism schools rising to the challenge, however, offer reasons to be optimistic about more journalists learning to treat data as a source.

Data visualization is an entire field which is developing alongside data growth. While the tools for visualization are interesting and fancy, visual results alone does not represent much value. It is the story and conclusions which can be drawn from the visual tools what actually matter. A good analogy is the bread preparation, the whole chain of events, starting from sowing the seeds and ending with backing the bread. The story represents the bread in this cycle, the final result, the product which can be published and consumed. The embrace of open source software and agile development practices, coupled with a growing open data movement, have breathed new life into traditional computer-assisted reporting. Collaboration across newsrooms and a focus on publishing data and code that show your work differentiate the best of today’s data journalism from the CAR of decades ago.

Data journalism can be created quickly or slowly, over weeks, months, or years. Either way, journalists still have to confirm their sources, whether they are people or data sets, and present them in context. Using data as a source would not eliminate the need for fact-checking, adding context, or reporting that confirms the ground truth. Just the opposite, in fact. Around the world, a growing number of data journalists are doing much more than publishing data visualizations or interactive maps. They are using these tools to find corruption and hold the powerful to account. The most talented members of this journalism are engaged in multiyear investigations that look for evidence that supports or disproves the most fundamental question journalists can ask: Why is something happening? What can data, related to narrative structure and expert human knowledge, tell about the way the world is changing?

In the hands of the most advanced practitioners, data journalism is a powerful tool that integrates computer science, statistics, and decades of learning from the social sciences in making sense of huge databases. At that level, data journalists write algorithms to look for trends and map the relationships of influence, power, or sources. As they find patterns in the data, journalists can compare the trends they discover to the shoe-leather reporting and expert sources that investigative journalists have been using for many decades, adding critical thinking and context as they go.


\subsubsection{Data Mining}
Data mining, an interdisciplinary subfield of computer science, is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. The goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use.

\begin{figure}[!ht]
\centering
\includegraphics[width=9cm]{1_data_mining}
\caption{A life cycle of data mining use case}\label{data_mining}
\end{figure}

The purpose of data mining is to conduct automatic or semi-automatic inquiries on large amounts of data in order to extract patterns such as groups of records, unusual data, dependencies. In order to perform operations of this scale usually special database techniques are needed. One of are them spacial indexes. This kind of indexed is created based on geospatial data structure, and it helps to increase the performance for queries such as finding the distance between two places. The kinds of patterns detected in a data mining task can be use for further analysis, machine learning, events prediction. Twitter introduced a curious research. Based solely on people tweets they have created an influenza pandemic prediction model. It is astonishing that a platform used for posting short text messages was used for predicting a biological phenomenon \mbox{\ref{data_mining}}. What if the research is done on a dataset strictly target to the analyzed field. Another successful data mining research was done for exploring specific cancer drugs. Before the research, the result was that the drug \mbox{(eg X)} was working in the 20\% of cases. After the genetic research based on data mining, was discovered the patient's genetic configuration of successful cases. As a result, it was possible to identify the people to whom the drug X will work for 100\%. The examples described above tells how useful data mining is and how many use cases it covers.

Data mining is not a solitary field. It takes advantages of more mature fields such as artificial intelligence and statistics. All these fields work hand in hand on problems such as pattern recognition and classification. Both fields have researched the understanding and applicability of neural nets and decision trees. To make it more clear, data mining its not an autonomous approach of solving this problems but more like an enhancement of traditional statistical models. The development of traditional statistical techniques was done based on elegant, mathematical based, theories that worked fine on small amounts of data. The problem is that the amount of generated data per day is increasingly drastically, mainly because the source are mostly machines. Log files, sensor output, data generated by computers. When it comes to analyzing it, the classical models will not apply. The need to analyze enormous data sets coupled with low cost computational power, allowed the development of new techniques based on brute-force exploration of possible present solutions.

Data mining started to become more and more popular because of the substantial contribution it can bake. It can be applied to control product costs or to increase the revenue. Popular organizations use data mining for managing all phases of the customer life cycle such as attracting new customers, increase sales from current customers, retaining good customers etc.
By profiling the preferences of good customers the organization can target customers with the same characteristics, or by having gathered the information about the customers that left, it can be deduced on what points to focus in order to keep the customers longer in the system. Data mining offers value across a broad spectrum of industries. The main industries which profits from data mining techniques are credit card companies and Telecommunications. Insurance companies and stock exchanges apply this technology to reduce the fraud. Companies active in the financial markets use data mining to determine market and industry characteristics as well as to predicting individual company and stock performance. Retailers are making use of data mining by analyzing the bills and find which products occur more frequently together. Using data mining results, the retailers rearrange products position in a manner that customers would buy more items.

\subsubsection{Web Crawling}
In context of OpenMedia, fetching the data is the first step in the system. Considering that the primary sources of raw data are the online public media, it would be a wise move to invest time in researching the concept of web crawling and how can it be used. What is the architecture of a proper designed crawler etc.

A web crawler (also known as a robot or a spider) is a system for downloading Web pages, usually in a batch manner. Web crawlers are used for a variety of
purposes. The most popular is being a part of a search engine. The main purpose is to fetch the pages, index its content such that in result users could find it the page by querying the key words found on the page. A related use is web archiving, where large sets of web pages are periodically collected and archived for posterity. Another use (the one relevant to the project) is data mining, where web pages are analyzed for statistical properties, or where data analytics is performed on them.

Crawling is an action which can be performed from anywhere, could run on potentially hundreds of threads, each of which loops through the logical cycle. These threads may be run in a single process, or be partitioned amongst multiple processes running at different nodes of a distributed system. A crawler frontier is the stack of URLs from which is selected the next visited page. The crawling process start by taking a URL from frontier stack and starts fetching the web page at that URL, generally using the HTTP protocol. The obtained result is saved into a temporary storage, a set of operations is performed on it, usually parsing the text, extracting new links to be added to the frontier. The text, with any tag information like terms in
boldface, is passed on to the indexer. For this purposes special tags and text enriching techniques are used to mark the important content on the web page. This is done solely with the purpose to make the web pages more likely be referred by the search engines. Next, a URL filter is used to determine whether the extracted URL should be excluded from the frontier based on one of several tests. For instance, the crawl may seek to exclude certain domains, for example .com  URLs, in this case the test would simply filter out the URL if it were from the .com domain. Web place certain portions of their websites off-limits to crawling, under a ROBOTS EXCLUSION standard known as the Robots Exclusion Protocol. This is done by placing protocol file with the name "robots.txt" at the root of the URL hierarchy at the site. The robots.txt file must be fetched from a website in order to test whether the URL under consideration passes the robot restrictions, and can therefore be added to the URL frontier. Due to the crawlers popularity, sitemaps protocol was adhered to the crawling concepts. It is an XML file which hints how it is better to crawl the specific web content provider. It includes the hierarchy of the web page, and metadata such as, what is the optimal frequency that the page should be crawled. It is a handy tool because it meets the crawler halfway, and guides the optimal ways of extracting data.

The web crawlers are needed because the web is not an information repository. It consist of millions of independent web content providers. Each one is providing their own service and information structured as the providers wants it. Web can be viewed as a agglomeration of content repository bounded together by a set of agreed-upon protocols and data representation formats. For instance Transmission Control Protocol, Domain Name Service, Hypertext Transfer Protocol etc.

In context of OpenMedia the crawling is planned to be done in an iterative way. Given that every article has an unique id by which it can be accessed, the only thing needed is to extract the latest article id (usually found on the main web page of the media portal). Having the latest article id, the URLs can be easily constructed and pushed into crawler's frontier. This gives the means of collecting all the needed raw data.

\subsection{Web Services}
Building complex systems is not a simple task and usually it consist of smaller logical parts that communicates trough interfaces. A modern distributed application usually runs on different machines. This is a wise choice to do, for multiple reasons. One is the easiness to troubleshoot problems. It is easier to understand how a system works once it is decoupled in multiple independent modules. A small logical unit can be understood faster and better and once it breaks down it is easier to fix it. But with great power comes great responsibilities. On one hand it might prove quite difficult and expensive to run instances of different applications, on the other hand it can scale efficiently when the demand is increased. A business will have a better profit if the product is available all the time, which means that it is recommended to pay the the extra money for the additional hardware. One more case might be to build an application that integrates many other public available services and APIs. In the end the point is that the applications should be able to communicate efficiently over the web. Various software are built in different programming languages, are running on diverse operating systems, hence a transparent communication model is needed and at the same time is language agnostic. That is how the web services protocols came to existence. During the time they have evolved into a set of communication standards that offered developers the opportunity to construct decoupled systems.

In order to define the standards, a set of rules are needed to be defined, such as:
\begin{itemize}
    \item How can a software perform a request to another system;
    \item What is the set of parameters that should be set in the request;
    \item What should be format of the request looks like;
    \item What are the logical parts that the request consists of;
    \item How should the response be represented;
    \item How should the errors be described.
\end{itemize}

As a result, on the market persist two approaches of constructing web services, SOAP and REST. Each approach have their strong points and weaknesses and both heavily relies on HTTP protocol, in case of SOAP it also supports other transport protocols.

\textbf{SOAP} is a messaging protocol that have the entire architecture wrapped around XML data representation. In a nutshell, it is a method of communication between two applications. An example of SOAP communication is represented in figure \mbox{\ref{soap}} The protocol specifies how exactly the HTTP headers should be encoded. A SOAP provider comes in hand with a WSDL file which represents the description of the web service. Things like the possible parameters and their formats, the structure of the message, what is the response format, how it can be correctly accessed. The communication via SOAP protocol is also done using an XML formated files. The structure of the the request and response is documented in the WSDL file and it is validated with the help of XSD schema.

\begin{figure}[!ht]
\centering
\includegraphics[width=11.5cm]{1_soap}
\caption{An example of SOAP communication}\label{soap}
\end{figure}


SOAP represents the next evolving stage between computer communication at he application layer. It was able to replace RPC technologies such as DCOM, CORBA, Java RMI. This reinstatement was highly needed because RPC technologies were brining a complex coupling to the programming language, which might prove a bad thing, as long as a panacea type programming language does not exist. On the other hand the SOAP calls are much slower comparing to native RPC applications. There tends to be firewall latency due to the fact that the firewall is analyzing the HTTP transport. SOAP calls are much more likely to get through firewall servers, since HTTP is typically Port 80 compliant, where other calls may be blocked for security reasons. Since HTTP requests are usually allowed through firewalls, programs using SOAP to communicate can be sure that the program can communicate with programs anywhere. SOAP is focuses on exposing pieces of application logic (not data) as services, platform operations. It aims for accessing named operations, each implement some business logic through different interfaces. An advantage offered by SOAP is the WS-Security which adds some enterprise security features. Supports identity through intermediaries, not just point to point (SSL). It also provides a standard for integrity.

\textbf{REST} is a simple stateless architecture that generally runs over HTTPS/TLS.This type of web service focuses on a reduced and very well defined amount of operations. The most common operations provided by a REST platform are CRUD. The flexibility is given by assigning resources their URI. The neat part is that it heavily relies on URLs. The REST philosophy is deeply entangled with HTTP protocol implementation, for instance the HTTP verbs GET, POST, PUT, DELETE, PATCH etc, are a part of REST RFC. Although it is just a set of guidelines and best practice, if implemented correctly the application avoids ambiguity. This is a blessing and a curse at the same time. Due to the fact that REST RFC is not a set of rigid requirement, developers tend to misinterpret the usage of some methods method. An example is overusing the "POST" method. Hence a lot of debates, discussions and even holly wars are held on this topic. The good part is that it doesn't need tedious descriptor files such as WSDL in order to describe a REST application. The documentation is usually done in textual manner, just like code documentation. REST gained a lot of popularity as being a simpler alternative to SOAP and WSDL-based web services. And the most viable example is the implementation of the entire Word Wide Web. One strong thing wielded by REST applications is that the message and response content can be delivered in any format. The most used are XML based format such as HTML, and for API platforms JSON is the most common and handy format, and it has lots of advantages against XML, such as readability, payload size, easy integrable with dynamic languages, it is data oriented.

Nowadays JSON is becoming the preferred format especially for RESTful APIs. Because of the format simplicity sometimes it gets harder to define the communication structure. Which is why JSON community is working now on an elegant format called JSON-api. It simplifies a lot things in terms of message structure. It resembles to WSDL only it is less restrictive and more intuitive. Another alternative for structuring the message format is HATEOAS. The purpose it aims is defining application state using hypertext.

\subsubsection{Text Processing}
The aim of text processing is to consume unstructured textual information, extract important numeric indices and make it accessible for various data mining, statistical algorithms. Information can be extracted to derive summaries for the words contained in the documents or to compute summaries for the documents based on the words contained in them. Hence a text analysis can be performed on clusters of words used in documents. Another approach could be to analyze documents and determine similarities between them or how they are related to other variables of interest. In a nutshell, text processing is an action of transforming text into numbers, and other metadata, eventually used in predictive data mining projects, machine learning, data warehousing, etc.

Unstructured text is very common, and in fact may represent the majority of information available to a particular research or data mining project. A relevant example of textual data analysis application would be automatic filtering of undesirable junk emails based on certain terms and words that are most likely to be met. In such manner the obnoxious mails can be discarded. It can be taken even further, such kind of applications could be used in a bigger systems for routing messages, based on email contents, to the appropriate department.

A powerful use case could be to process the contents of a particular Web portal. For example for extracting relevant specific block, and determining the most frequently mention word, thus deriving the most important terms of the web application. its easy to observe how this kind analysis could deliver valuable business intelligence. This kind of use case resembles very much with system requirements of the OpenMedia platform. Given that the aim is to crawl all the possible articles of the media portals, and perform data mining algorithms on it. The final result is a set statistics and data visualization toolkit

Another curious aspect of data processing is that there are two angles of processing textual documents. First is many data to few documents, and second is small data from lots of documents. For instance, on case is performing complex text analysis tasks on few lengthy resources (like books) and the another is extracting key values concepts from tweeter messages that have a size limit of 140 characters. The second case is more compliant to statistic algorithms by the fact that the amount of data sources is much bigger. This means that OpenMedia is a perfect playground for applying various statistic models.

One think that should be taken into consideration is the data indexing. Indexing is a feature usually provided by the modern databases. The idea behind it is to create additional data, resembling a hash map data structure, that allows to search data much faster by a specific field. For instance three is a classic SQL database with a "user" schema that has a field called "name". If the system using the database is frequently searching for "user" entity by its "name" then it is recommended to perform indexing on the table. This will add additional data on the disk, instead the search queries by "name" would be much faster. When dealing with big chunks of textual data indexing by each word my not prove a sensible option. Instead some tricks can be applied, like removing certain characters, words bounded by some length, frequently used stop words like "the", "a", "of" etc. Another idea would be a custom indexing task that would detect synonyms and index as the same entity when a pair is found. One more thing would be to remove the rarely used words that do not offer much information about the text.

\subsubsection{Lexical Analysis}
Open Media project works with articles fetched from the available public sources. Big chunks of text does not represent a big value. The reasons are that a spoken language such as English or Romanian (in this particular case) are vast and complex. More than that they might prove to be ambiguous. A simple task like determining the part of speech of a specific word might prove a complex action. The same word might have different meanings in different circumstances. Which is why a probabilist model based on machine learning has to be constructed in order to detect word's context and determine the part of speech it belongs to. Textual data enriched with metadata such as a part of speech is proved to be useful for advanced text analysis. A trivial case is for eliminating the stop words. There are different approaches for this problem. First one is having a dictionary with stop words and check every word if it is a part of the record. The second approach is to assume that all the stop words have a length lesser than four. The issue is that there are a lot of exception thus not feasible to cluster the words solely on number of characters. The approach that will have an acceptable error rate and will work quite fast (in comparison with a dictionary check) would be, first to detect the part of speech, followed by words elimination that refers to the stop words.

The process described above is one of the many kinds of text preprocessing tasks that involves lexical analysis. Many of theses processes work hand in hand with another. Usually one can only operate on a dataset that resulted from another NLP operation. Here are listed the operations regarding lexical analysis from the family of natural language processing:
\begin{itemize}
    \item Tokenization;
    \item Part of speech tagging;
    \item Lemmatisation;
    \item Text Segmentation;
    \item Chunker;
    \item Syllabification;
    \item Stemming.
\end{itemize}

\textbf{Natural Language Processing} is a field of computer science, artificial intelligence and computational linguistics which studies the interactions between computers and human languages. One of the main challenge of this field involves natural language understanding, where computers are able to extract meaning out of human language input. The classic approach of solving this problem was by hand coding a big set of rules which the machine has to follow. In such way decision trees models were created, basically a lots of "if else" statements. But unfortunately a natural language is a rather complex concepts, and catching all ambiguities makes the decision tree unmaintainable. In the end the traditional decision trees usually resulted in a rather primitive solution. Modern NLP uses lots of machine learning algorithms under the hood. The idea is to apply the learning on a set of corpora where a corpus is a set of documents, sentences which have been hand-annotated with the correct values to be learned. The annotations representing metadata such as part of speech, the word root, synonyms etc. Having a probabilistic model gives the advantage that in a specific situation the solutions can be multiples, with different chances. Usually such type of results are better suited as being a part from a larger system.

\textbf{Tokenization} is a part of NLP field. It represents the process of breaking a string into words, symbols, phrases or other meaningful elements that can be called tokens. Typically the segmentation is done at the word level. The traditional implementation relies on simple heuristics. Tokens are usually separated by whitespace or punctuation characters. Most of Latin alphabet based languages use inter-word spaces (eg New York-based). For such kind of languages the solutions is straightforward. The grade of complexity is more increased for languages such as Ancient Greek, Chinese, Thai etc.

\textbf{Lemmatization} is another important part of NLP. The idea behind it is to reduce the word to a canonical form. Similar words, or with the same root would have the same result after passing the lemmatisation process. This will give the opportunity to operate with a group of words from the perspective of a single entity. The stemming process is rather similar. The difference is that stemming can be applied to an independent word, while lemmatization works based on the word context, meaning that it has a more complex algorithm in the background. The process already requires additional metadata, for instance, the word's part fo speech. Even though the stemming algorithm is more primitive, due to the fact that it doesn't discriminate the word meaning, the implementation is much simpler. This fact gives an upper hand for a data analysis process, especially when the computational speed is a bottleneck. Lemmatization is when words like "traveling", "traveled" will be recognized as the same word "travel". This aspect is crucial in context of OpenMedia, if the words are not brought to a canonical form, then the frequency analysis would have an increased error rate, that would lead to an unreliable platform not likely to be used by users.

\subsubsection{Data Storage}
The whole idea of computer science is wrapped around of ways of manipulating data. From the very start engineers had issues with finding ways to store data. During the time, the hardware evolved and nowadays the disk space does not represent a problem anymore. The actual challenge is how to make interaction with data as efficiently as possible. Interaction represents means of reading, and querying data, effectively saving it on the disk, keeping it consistent and avoid data loss. What if data is related to other data types. How to  implement the relationship between the data. How to make possible for multiple users to read and write at the same time. This are the actual problems which are confronted in computer science.

The classical solution to this problem are the RDBMS approaches. It is a common choice for the storage of information in new databases used for financial records, manufacturing and logistical information, personal data, and other applications since the 1980s. Relational databases have often replaced legacy hierarchical databases and network databases because they are easier to understand and use. The relational databases rely on SQL which is a special-purpose language designed for managing data. It is used to query, insert, update and modify data. Because the amounts of data in computer science, until recently, were not so big. RDBMS were and still are an irreplaceable solutions for managing efficiently relatively small amounts of data. The RDBMS philosophy is built around ACID principle, Atomicity, Consistency, Isolation and Durability. The combination of this four principles has granted such a big success to relational databases.

As mentioned above RDBMS is widely used for lots of applications and successfully solves problems and there hasn't been a better alternative on the market. The competition is applied to different implementations of databases, such as PosgreSQL, MySQL, OracleSQL, MsSQL, MariaDB. All of them are quite similar, and each has its strong and weak points. The actual problem appears when the Big Data started to get more an more popular. Unfortunately the classical database approach was not enough for the constantly data increase. RDBMS enforces a well defined schema, as a result it gets slower and unmanageable when the amount of data gets bigger.

What developers decided was to loosen up one of the ACID principle and create new brand of database which have a different structure and would allow storing and working with big amounts of data. This is how the term BASE principal came to life. Basically Available, Eventually Consistent. BASE concept supports the idea of network partitioning, which means that the database will always have a response disregarding the amount of requests at the given time. The catch is what kind of response should it have in case of multiple access to the database. Two solutions were proposed. First one is that the database should always return a result even though it is not up to date. The second solutions is to inform the database user that the service is not available for now. Both solutions have their own applications. Choosing which one to use depends entire on what is better suited for business. The idea of choosing the database model is also presented by CAP theorem. CAP states that when choosing a database you can choose only two of the three features. These are Consistency, Availability and network Partition tolerance. In figure \mbox{\ref{cap}} is illustrated in more details the CAP and databases categorized based on theorem.

\begin{figure}[!ht]
\centering
\includegraphics[width=17cm]{1_cap}
\caption{An illustration of CAP Theorem}\label{cap}
\end{figure}

Along with the implementation of conceptual new database the term NoSQL started to spread. The term came from the fact that the new databases were not using SQL for data management. In reality there are more types of conceptual database, and covering all of them under the same umbrella seems too ambiguous.
The most widely used types of databases, not considering RDBMS, are described bellow.

\textbf{Document Based Database} is a new approach of database management. It is used in usual English sense of a group of data that encodes some sort of user-readable information. This contrasts with the value in the key-value store, which is assumed to be opaque data. The basic concept that makes a database document-oriented as opposed to key-value is the idea that the documents include internal structure, or metadata, that the database engine can use to further automate the storage and provide more value.

Document databases contrast strongly with the traditional relational database (RDBMS). Relational databases are strongly typed during database creation, and store repeated data in separate tables that are defined by the programmer. In an RDBMS, every instance of data has the same format as every other, and changing that format is generally difficult. Document databases get their type information from the data itself, normally store all related information together, and allow every instance of data to be different from any other. This makes them more flexible in dealing with change and optional values, maps more easily into program objects, and often reduces database size. This makes them attractive for programming modern web applications, which are subject to continual change in place, and speed of deployment is an important issue. The current most popular implementation of such type of database is MongoDB and CouchDB.

\textbf{Column Based Database} is a database management system that stores data tables as sections of columns of data rather than as rows of data. In comparison, most relational DBMSs store data in rows. This column-oriented DBMS has advantages for data warehouses, customer relationship management systems, and library card catalogs, and other ad hoc inquiry systems where aggregates are computed over large numbers of similar data items.

It is possible to achieve some of the benefits of column-oriented and row-oriented organization with any DBMSs. Denoting one as column-oriented refers to both the ease of expression of a column-oriented structure and the focus on optimizations for column-oriented workloads. This approach is in contrast to row-oriented or row store databases and with correlation databases, which use a value-based storage structure. Such type of database implementations are BigTable, Casandra.

\textbf{Key Value Database} use the associative array (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection. The key-value model is one of the simplest non-trivial data models, and richer data models are often implemented on top of it. The key-value model can be extended to an ordered model that maintains keys in lexicographic order. This extension is powerful, in that it can efficiently process key ranges. Example of such type of database implementations are Redis, Memcache, Voldemort.

\subsubsection{Hadoop Mapreduce}
In the last 10 years terms like Big Data, NoSQL, Hadoop, mapreduce were so frequently discussed that some of them became bloated. For instance, what is Big Data? how "Big" is the data? In 2003 Google released a paper about Google File System, and later in 2004 a mapreduce paper. Being a company that works with indeed extremely big data, sizes in terms of petabytes and more, they have started to search for solutions how to manage such amounts. Due to the fact that the computational power was getting cheaper by day, they have come up the idea of building file systems that would be able to run on a cluster of computers. In the path of finding the methods to operate with the huge amounts of data they have seen that the traditional RDBMS database engines would not fit their requirements, so they've came up with an alternative. On 2004 they started working on a new kind of data base structured on a different concept, BigTable. A database that exchanges the classical ACID rules with scalability, availability and other positive aspects that would solve their problem. Google is not the only company working with huge amounts of data, resulting in other technologies that were developed based on similar concepts. For instance Amazon came up with DynamoDB, a database that works on the same principle as BigTable, instead it was focused on offering availability instead of consistency (a customer should always be able to buy even though the product is not available in stock). Another example is Yahoo, who came up with Apache Hadoop, a great technology released under the Apache open source license. It is a files system that can work on multiple computers and indeed successfully solve the problem of processing big amounts of data.

After the multiple successful stories presented by the leading companies, the terms Hadoop, Big Data, NoSQl, started to become popular and spread like a fever. Lots of companies started using this technologies, because you have to follow the cool kids. The problem was that companies, especially new one, were not making the business smart decisions when choosing the technologies. For instance building an application using a schemaless database. After making the wrong decision it was realized that the application structure actually needs a schema structure. The result was the application layer managed the schema consistency. This is an additional layer of complexity that can be handled by a traditional RDBMS engine. A analogous story is related by the Olery company \cite{mongo_to_postgres}. Many startups started to use the newly discussed solutions, forming communities, thus the deceiving labeling started. Everything that is not RDBMS is NoSQL, a term that was intended to be used as a twitter hashtag now covers under its umbrella all the databases that does not rely solely on ACID principles. The same situation is with Big Data. What is the actual size of the data required for being called "big". There are lots of speculations on this topic.

\textbf{Hadoop} is a framework used for distributed storage and distributed processing of large scale of data sets. It can benefit from cheap computation power by running on a mediocre computer cluster. The platform is designed by taking into consideration the hardware failure. It is natural that at some point in time a machine from the cluster will stop running. Hadoop embraced this problem and solved it at the software level. Although Hadoop is best known for mapreduce and its distributed filesystem (HDFS, renamed from NDFS), the term is also used for a family of related projects that fall under the umbrella of infrastructure for distributed computing and large-scale data processing. As the Hadoop ecosystem grows, more projects are appearing, not necessarily hosted at Apache, which provide complementary services to Hadoop, or build to add higher-level abstractions.

Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.

HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode. In figure \mbox{\ref{hadoop_architecture}} is represented the HDFS architecture.

\begin{figure}[!ht]
\centering
\includegraphics[width=13cm]{1_hadoop_architecture}
\caption{Hadoop architecture, \cite{data_mining_image}}\label{hadoop_architecture}
\end{figure}

The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely necessary to be the case.

\textbf{Mapreduce} is a rather primitive operation. It has a brute-force like approach. It is a batch query processor, and the ability to run a query against an entire dataset and get a result in reasonable amount of time is an astonishing thing. It opens an entire new perspective of processing data. The algorithm works in the following way. The user specify a map function which is applied to every record. The goal is to emit a key value pair in order to generate a set of intermediate key/value pares. The reduce function merges all intermediate values associated with the same intermediate key. Many real problems can be modeled under this concept. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. In listing \mbox{\ref{mongo_example}} is an example of mapreduce implemented for MongoDB. MongoDB API has a native mapreduce feature.

\lstinputlisting[language=Java, caption={Mapreduce example in MongoDB}, label=mongo_example]{../src/map_reduce_example.js}

\subsubsection{Data Visualization}
Human has a long history with basic data visualization, and data visualization is still a hot topic today. The history of visualization was shaped to some extent by available technology and because of the pressing needs of time, they include: primitive paintings on clays, maps on walls, photographs, table of numbers, these are all some kind of data visualization, although nowadays this kind of art may not be called data representation. Visualization is the graphical presentation of information, with the goal of providing the viewer with a qualitative understanding of the information contents. It is also the process of transforming objects, concepts, and numbers into a form that is visible to the human eyes. Beyond that, well designed data visualization tools aims to deliver a meaningful message to the consumer.

Big data is creating unprecedented opportunities for businesses to achieve deeper, faster insights that can strengthen decision making, improve the customer experience, and accelerate the pace of innovation. But today, most big data yields neither meaning nor value. Businesses are so overwhelmed by the amount and variety of data that they struggle just to store the data, much less analyze, interpret, and present it in meaningful ways.

The trend toward visualization based data discovery tools is worth exploring by any business that seeks to derive more value from big data. The potential business benefits are immense, and data governance best practices can be used to help ensure a safe transition. As demonstrated by three usage examples from TIBCO Software, the world’s second largest data discovery vendor, real world applications of visualization based data discovery tools are already delivering greater customer and market insights to businesses around the world.

Data analytics and visualization are not new. For decades, businesses have collected data, analyzed it using a variety of BI tools, and generated reports. Businesses are finding that this traditional reporting process does not work nearly as well for big data, and certainly is not sufficient to capture the potential value that big data represents. The primary challenges stem from what are commonly termed the “three Vs” of big data: volume, variety, and velocity. Most traditional reporting and data mining tools cannot handle the vast volume of big data, although the variety and velocity of the data often present even greater challenges. Data can be represented in various way, it can have as many forms as a human brain can imagine. Even so there are already steady classical patterns for data visualization which are well known by common humans, and more important they are easy to read.

Simple bar charts are the most common form of data visualizations. Typically they only display different quantities of single variable data. However other variations, such as stacked bar charts or multi set bar charts can be used to compare multiple variables using bars. Line graphs are created by plotting points on a Cartesian grid, usually with the horizontal axis representing time. They are very powerful because without looking at the specific data, they show how a variable develops over time (from left to right). Stacked area charts are similar to line charts, however with the added value of filled areas. The data that is stacked adds up to a total of all variables combined. For example a business might use stacked area charts to visualize their total income, with each stacked area a different income channel. Pie charts are the most common visual used to compare proportional data. They give viewers a very quick understanding of the distribution of the data. Pie charts are not useful when comparing many pieces of data with relatively close values. Ring charts are used to visualize the distribution of a data set. The advantage is they compare similar data sets. The alternative would be to place multiple pie charts next to each other, this can also be viewed as a space-saver. Scatterplots are created by plotting independent points on a Cartesian grid. Scatter plots are often used to find the relationship between data or to reveal information such as trends within the data which are not easily visible when in a table. Only works with two dimensional data. Bubble charts display more dimensions of data by varying size (or also color, texture, etc) of the bubbles. It therefore can display multiple dimensions of data in a two dimensional display. Tree diagrams are often used when wanting to represent the strict hierarchy of data. They are most often used to represent strict hierarchies such as family trees or how data is stored in a computer system. Diagram maps visualizations are used to primarily represent the connections between different nodes or points. Their purpose is to show which points are connected to each other. Common examples of diagram maps are metro maps and social network visualizations. Maps are used when the data is related to a specific location (for example a city, or country). The advantage is that their spatial representation directly relates to a real world situation. However at times it can be difficult to read.


\subsubsection{Modern Web Application}
Web applications are heavily using HTTP protocol as means of transporting data. HTTP is a stateless protocol. For every request made by a client a TCP socket is opened. The HTTP server receives a request that is handled by the application layer. When the response is sent back to the client, the TCP socket is closed and the transaction ends. The whole chain of events is repeated basically at every user interaction. The result is that a simple web application has a stateless behavior. The application layer of a web applications aims to get rid of statelessness. In 2004 the concept of web 2.0 surfaced. Javascript started to become more popular because it gave the power to animate the pages and create a more humane UX. The magic was behind the AJAX technology. The concept introduced by AJAX was making a web page run asynchronous requests and make live partially DOM changes. Developers could create web applications which did not require full page reload at while interacting with a web page. Successfully implementation of this concepts are Facebook, Gmail, Twitter etc. AJAX, JQuery and other Java Script technologies brought web applications one step closer to the desktop applications experience.

Nowadays the single page applications are becoming a hot topic. The main reason is that they are able to offer more native application like experience to the user. This is hard to do with other approaches. Supporting rich interactions with multiple components on a page means that those components have many more intermediate states. Server side rendering is hard to implement for all the intermediate states. Small view states do not map well to URLs.

Single page applications are distinguished by their ability to redraw any part of the UI without requiring a server round trip to retrieve HTML. This is achieved by separating the data from the presentation of data by having a model layer that handles data and a view layer that reads from the models. Interaction with the single page application often involves dynamic communication with the web server behind the scenes.

Here are enumerated a set of technologies that helps of building single page applications:
\begin{itemize}
    \item Ember;
    \item Angular;
    \item React (recently introduced by Facebook\cite{react});
    \item Meteor;
    \item Marionette.
\end{itemize}

Due to the fact that single page applications have a rich functionality, they also include a complex architecture. For instance in figure \mbox{\ref{ember_architecture}} is illustrated the conceptual structure of Ember framework. It is hard to wrap the head around the structure, but once there is a basic understanding of the logical layers, building applications is a joy for a developer.

\begin{figure}[!ht]
\centering
\includegraphics[width=10cm]{1_ember_architecture}
\caption{Ember framework architecture}\label{ember_architecture}
\end{figure}

Until now everything discussed was related to building the front part of a modern web application. But a web application consist also from the backed part. The HTTP application that listens client requests. For building one there are a lot of frameworks which easily allows to scaffold a prototype. MVC based frameworks are powerful and provides lots of functionalities out of the box and the good thing is that the majority of frameworks are mature and stable. Here are a list of frequently used solutions:
\begin{itemize}
    \item Ruby on Rails;
    \item Django;
    \item ASP .NET;
    \item Symfony.
\end{itemize}

The mentioned technologies have huge stacks that sometimes are not needed when building a smaller application, or scalable one. Besides for building modern web applications where the client application is developed in a Javascript framework, means that the "V" (view) part from MVC is not needed anymore. Plus there are already on the market lightweight web technologies such as Sinatra, Flask, Node (in combination with express library). This type of application can serve just as good. In case if new module is required by the application, it can be easily added to the micro-framework stack. In ruby this is done by using gemfiles (gems are libraries in Ruby language) were gems can be easily added and installed effortlessly.

\subsubsection{Web Sockets}
WebSocket is a protocol providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C.

WebSocket is designed to be implemented in web browsers and web servers, but it can be used by any client or server application. The WebSocket Protocol is an independent TCP-based protocol. Its only relationship to HTTP is that its handshake is interpreted by HTTP servers as an Upgrade request. The WebSocket protocol makes more interaction between a browser and a website possible, facilitating live content and the creation of real-time games. This is made possible by providing a standardized way for the server to send content to the browser without being solicited by the client, and allowing for messages to be passed back and forth while keeping the connection open. In this way a two-way (bi-directional) ongoing conversation can take place between a browser and the server. The communications are done over TCP port number 80, which is of benefit for those environments which block non-web Internet connections using a firewall. Similar two-way browser-server communications have been achieved in non-standardized ways using stop-gap technologies such as Comet. In OpenMedia WebSocket protocol is used to communicate between platform modules.

The WebSocket protocol is currently supported in most major browsers including Google Chrome, Internet Explorer, Firefox, Safari and Opera. WebSocket also requires web applications on the server to support it. WebSocket reduces latency. For example, unlike polling, WebSocket makes a single request. The server does not need to wait for a request from the client. Similarly, the client can send messages to the server at any time. This single request greatly reduces latency over polling, which sends a request at intervals, regardless of whether messages are available. WebSocket makes real-time communication much more efficient. Polling can always be used (and sometimes even streaming) over HTTP to receive notifications over HTTP. However, WebSocket saves bandwidth, CPU power, and latency. WebSocket is an innovation in performance. It is also is an underlying network protocol that enables to build other standard protocols on top of it. It is also a part of an effort to provide advanced capabilities to HTML5 apps in order to compete with other platforms.

